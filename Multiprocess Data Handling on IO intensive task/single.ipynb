{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Time to Python Multi-processing\n",
    "I want to share something I learned about Python multi-processing on handling middle size data on a single machine.\n",
    "\n",
    "By middle size, I mean GB level data. In my case, that is 13.2GB of data, which is small enough to load into memory but once load into memeory, you may not have enough memory to run any algorithms.\n",
    "\n",
    "Let's dive into it.\n",
    "\n",
    "# Problem Description\n",
    "I have 13.2GB of data consisted of 7481 binary files. Each file is about 1.81M.\n",
    "\n",
    "My task is simple:\n",
    "    1. Read the file into a Python data structure\n",
    "    2. Process the data to an image\n",
    "    3. Write the image to disk\n",
    "    \n",
    "## File Format\n",
    "The file stores a collection of vectors. Each vector has 4 elements: x, y, z, r\n",
    "\n",
    "Each element is a float32, so the size of a vector is 16 bytes.\n",
    "\n",
    "There is no delimiters between 2 vectors. Therefore, if the file size is 32 bytes, it has and only has 2 vectors. If the size of a file is not dividible by 16 bytes, it is not a valid file.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_velodyne_data(file_path):\n",
    "    \"\"\"\n",
    "    Read velodyne binary data and return a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # First, check the size of this file to see if it's a valid velodyne binary file\n",
    "    size = os.stat(file_path).st_size\n",
    "    if size % 16 != 0:\n",
    "        raise Exception('The size of '+file_path+' is not dividible by 16 bytes')\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Allocate memory for numpy array\n",
    "        velodyne_data = np.empty(shape=(size//16, 4), dtype=np.float32)\n",
    "\n",
    "        # Read the data, 16 bytes each time\n",
    "        i = 0\n",
    "        reader = BufferedReader(f)\n",
    "        while reader.peek(16):\n",
    "            read_bytes = reader.read(16)\n",
    "            velodyne_data[i] = np.frombuffer(read_bytes, dtype=np.float32)\n",
    "            i += 1\n",
    "\n",
    "        # Check whether correct amount of bytes were read\n",
    "        if i != size/16:\n",
    "            error = ' '.join(['The file size is', str(size), ', but', str(i), 'bytes were read'])\n",
    "            raise Exception(error)\n",
    "\n",
    "        return velodyne_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's process the data\n",
    "Each file is a velodyne point cloud scan\n",
    "\n",
    "This is an sample processed image. This a a birdview of the lidar data\n",
    "<img src=\"birdview.jpg\" style=\"width: 600px; height: 600px\"/>\n",
    "\n",
    "The image is 1600 x 1600 pixel. Each pixel represent 10cm x 10cm of space. The range of the lidar in the x and y direction is 80m.\n",
    "\n",
    "Where x is left and right, y is front and back.\n",
    "\n",
    "We have 3 channels for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bird_view_map(velodyne_data):\n",
    "    \"\"\"\n",
    "    Implements the method in https://arxiv.org/pdf/1611.07759.pdf\n",
    "    :param velodyne_data: a list of velodyne cloud points\n",
    "    :return: 2D image with 3 channels: height, intensity and density\n",
    "    \"\"\"\n",
    "    bird_view = np.zeros(shape=(1600, 1600, 3), dtype=np.float32)\n",
    "    for point in velodyne_data:\n",
    "        x = point[0]\n",
    "        y = point[1]\n",
    "        z = point[2]\n",
    "        r = point[3]\n",
    "        # if (-40 <= x <= 40) and (-40 <= y <= 40):\n",
    "        xi = 800 - np.int(np.ceil(x/0.1))\n",
    "        yi = 800 - np.int(np.ceil(y/0.1))\n",
    "        if -z > bird_view[yi][xi][0]:\n",
    "            bird_view[yi][xi][0] = -z\n",
    "            bird_view[yi][xi][1] = r\n",
    "            bird_view[yi][xi][2] += 1\n",
    "\n",
    "    # todo: normalize birdview with the real method\n",
    "    bird_view[:,:,0] = np.interp(bird_view[:,:,0], xp=(np.min(bird_view[:,:,0]), np.max(bird_view[:,:,0])), fp=(0, 255))\n",
    "    bird_view[:,:,1] = np.interp(bird_view[:,:,1], xp=(0, 1), fp=(0, 255))\n",
    "    bird_view[:,:,2] = np.interp(bird_view[:,:,2], xp=(np.min(bird_view[:,:,2]), np.max(bird_view[:,:,2])), fp=(0, 255))\n",
    "    return bird_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single processed version used 143.17335605621338 seconds\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from io import BufferedReader\n",
    "import time\n",
    "\n",
    "\n",
    "paths = glob('data/raw/*.bin')\n",
    "t = time.time()\n",
    "for path in paths:\n",
    "    data = read_velodyne_data(path)\n",
    "    view = bird_view_map(data)\n",
    "    cv2.imwrite(''.join(['data/processed/', os.path.basename(path)[:-4], '.png']), view)\n",
    "used = time.time() - t\n",
    "print('Single processed version used', used, 'seconds to process', len(paths), 'files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we optimize this program?\n",
    "Notice that we have 2 functions here:\n",
    "1. __read_velodyne_data__, which does the input.\n",
    "2. __bird_view_map__, which does the computation.\n",
    "\n",
    "Before we get into any of those parallelism things, the first thing is always try to optimize our individual functions. Because if your functions are slow, your parallel code is just slow code on more cores.\n",
    "\n",
    "We are not going to use our intuition or experiences to just staring at our code and hope something could happen.  \n",
    "\n",
    "We are going to employ profiling tools to precisely measure our code performance.\n",
    "\n",
    "### Profile the code\n",
    "The traditional cProfile won't be useful here because it only profiles the code at a function level. It only tells you how much time each function call uses. Let's say you know that function __f1__ used 80% of your program time. You still don't know which line in __f1__ cost you so much time.\n",
    "\n",
    "That's why we need a line by line profiler [line_profiler](https://github.com/rkern/line_profiler) by [Robert Kern](https://github.com/rkern).\n",
    "\n",
    "Please look at my __README.md__ and line_profiler's README for profiling instructions.\n",
    "\n",
    "### Now we know\n",
    "Now we know that __while reader.peek(16)__ is the bottomneck of __read_velodyne_data__ function.\n",
    "\n",
    "__peek__ is unnecessary because we are going to read the data anyway. Therefore, we can change this function to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_velodyne_data_quick(file_path):\n",
    "    \"\"\"\n",
    "    Read velodyne binary data and return a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    # First, check the size of this file to see if it's a valid velodyne binary file\n",
    "    size = os.stat(file_path).st_size\n",
    "    if size % 16 != 0:\n",
    "        raise Exception('The size of '+file_path+' is not dividible by 16 bytes')\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Allocate memory for numpy array\n",
    "        velodyne_data = np.empty(shape=(size//16, 4), dtype=np.float32)\n",
    "\n",
    "        # Read the data, 16 bytes each time\n",
    "        i = 0\n",
    "        reader = BufferedReader(f)\n",
    "        read_bytes = reader.read(16)  # As you can see here, we read directly and check if read_bytes has values.\n",
    "        while read_bytes:\n",
    "            velodyne_data[i] = np.frombuffer(read_bytes, dtype=np.float32)\n",
    "            read_bytes = reader.read(16)\n",
    "            i += 1\n",
    "\n",
    "        # Check whether correct amount of bytes were read\n",
    "        if i != size/16:\n",
    "            error = ' '.join(['The file size is', str(size), ', but', str(i), 'bytes were read'])\n",
    "            raise Exception(error)\n",
    "\n",
    "        return velodyne_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's run it for 100 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better version used 131.4069118499756 seconds\n",
      "It is  11.766444206237793 seconds faster for 100 files\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "paths = glob('data/raw/*.bin')[:100]\n",
    "for path in paths:\n",
    "    data = read_velodyne_data_quick(path)\n",
    "    view = bird_view_map(data)\n",
    "    cv2.imwrite(''.join(['data/processed/', os.path.basename(path)[:-4], '.png']), view)\n",
    "used2 = time.time() - t\n",
    "print('Better version used', used2, 'seconds')\n",
    "print('It is ', used - used2, 'seconds faster for', len(paths), 'files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4% Performance Gain\n",
    "As you can see, conversativly speaking, it is about 5 seconds faster for 100 files. This is about 4% faster.\n",
    "\n",
    "I don't know about you, but I am impressed. Just by changing a single line of IO, we can save 4% of time.\n",
    "\n",
    "For our data, we have 7821 files, we will save 391.05 seconds in total. \n",
    "\n",
    "Just image if you have much more data, 4% is incredible optimization.\n",
    "\n",
    "### Multi-process\n",
    "Because notebook doesn't work well with multi-process code, we will dicover the multi-process world in an ordinary Python script.\n",
    "\n",
    "Please see __multi.py__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
